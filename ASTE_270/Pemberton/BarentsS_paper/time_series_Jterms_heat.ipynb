{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6ddce1-8cd0-4fc2-b995-22a5abbefc19",
   "metadata": {},
   "source": [
    "# load the packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc15ad8f-3634-4c33-be7b-15d1c530cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "import gsw\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "# import existing python files\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "\n",
    "# add rdmds reading functions to path\n",
    "sys.path.append(\"/home/mmurakami/MITgcm/MITgcm_c68r/MITgcm-checkpoint68r/utils/python/MITgcmutils/MITgcmutils/\") # go to parent dir\n",
    "from mds import *\n",
    "\n",
    "# add the other files\n",
    "sys.path.append(\"/home/mmurakami/crios_backups/an_helper_functions\")\n",
    "from read_binary import *\n",
    "from calc_UV_conv_1face import calc_UV_conv_1face\n",
    "from calc_mskmean_T_mod import calc_mskmean_T_mod\n",
    "from mk3D_mod import mk3D_mod\n",
    "from aste_helper_funcs import *\n",
    "from timing_functions import *           # ts2dte, get_fnames, etc.\n",
    "from binning import *                    # bin_array, create_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8da6f8-1381-40e7-a012-b4e81539d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1610a513-dd00-4c45-a70d-fa517408f0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,) (50, 1350, 270)\n",
      "hf1 (1350, 270)\n",
      "(1, 1350, 270)\n",
      "LwetC2d 146614\n",
      "LwetC 4833023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run /home/mmurakami/crios_backups/ASTE_270/prep_grid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cbffb-739b-43b0-9db8-315abf784d61",
   "metadata": {},
   "source": [
    "# load the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2015777-19ac-4c49-abb3-6995998d66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basin we want\n",
    "path = \"/home/mmurakami/crios_backups/ASTE_270/offline_binning/sample_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4023eb-be4b-4b9e-8360-92b2b5857d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364500\n"
     ]
    }
   ],
   "source": [
    "iB = 6    # read from BarentsSea\n",
    "    \n",
    "# mymsk below defines as all Arctic down to Fram Strait and BSO but not GINs Seas\n",
    "mymsk = mskBasin.copy()\n",
    "\n",
    "# Create a boolean mask for elements that are 6 or less\n",
    "mask = mymsk == 6\n",
    "\n",
    "# Set elements that are greater than 6 to np.nan\n",
    "mymsk[mask] = 1\n",
    "mymsk[~mask] = np.nan\n",
    "\n",
    "# Get the number of points where mskBasin is 6 or less\n",
    "npoints = np.count_nonzero(mymsk)  # Count the number of True values in the mask\n",
    "print(npoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a67a58b-9146-4076-b78e-c5847cd7ed60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1350, 270)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymsk3 = np.tile(mymsk[np.newaxis,:,:],(nz,1,1)) * mygrid['hFacC']\n",
    "mymsk3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4f7e9-0161-4c94-a875-ddd5f331bc6c",
   "metadata": {},
   "source": [
    "# timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b00d5f-b355-47f4-94af-5ad286fd8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of the time steps we want to read\n",
    "# use ts2dte to get december 2014\n",
    "# first make an array of filenames\n",
    "dt_aste = 600\n",
    "startyr = 2002\n",
    "endyr = 2019\n",
    "\n",
    "# all the filenames in the system\n",
    "fnames = get_fnames(dt_aste,startyr,endyr)\n",
    "\n",
    "# ocean and ice\n",
    "AB_gT=0\n",
    "AB_gS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c483d2f-05e5-42d5-81a4-9430d80c92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbdcad04-8d3a-4643-b80e-9f02d3e46c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = list(np.arange(2003,2018,1))  # 15 year period\n",
    "years = [2003,2004]\n",
    "years = [str(i) for i in years]\n",
    "years = np.array(years)\n",
    "# years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749aa26a-7bc0-44cb-bb6c-8aef5711b0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write the datetimes for the later period\n",
    "times = {}\n",
    "\n",
    "for year in years:\n",
    "    times[year] = np.arange(1,13,1)   # write all the months for this example 5-year period\n",
    "\n",
    "tsstr,datetimes = get_tsteps(times,fnames,dt_aste,startyr,1,1)\n",
    "tsstr.shape\n",
    "datetimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56e9dd4-1571-44d6-ace8-308141ee50a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for one year\n",
    "# tsstr = tsstr[:7]\n",
    "# datetimes = datetimes[:7]\n",
    "tsstr = tsstr[6:13]\n",
    "datetimes = datetimes[6:13]\n",
    "datetimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eaeabcc-3fe3-470e-a9c8-1c6411ff02aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'datetimes' is a list of datetime objects\n",
    "dt = [(datetimes[i+1] - datetimes[i]).total_seconds() for i in range(len(datetimes) - 1)]\n",
    "dt = np.array(dt)\n",
    "dt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db750f-36dc-4ab1-b93f-ba894de3648e",
   "metadata": {},
   "source": [
    "# create the J terms\n",
    "\n",
    "In general, when we create the time-mean for these terms we are interested in the mean weighted by depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15c671-ebd1-45f0-ba79-fe532becb596",
   "metadata": {},
   "source": [
    "### For heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "279d38a0-573b-4d8e-931f-560b3fd53760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 50, 1350, 270)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with the tendency\n",
    "# read thetadr\n",
    "file_name = 'budg3d_snap_set2'\n",
    "meta_budg3d_snap_set2 = parsemeta(dirIn + file_name + \".\" + tsstr[0] + \".meta\")\n",
    "fldlist = np.array(meta_budg3d_snap_set2[\"fldList\"])\n",
    "varnames = np.array([\"THETADR\"])\n",
    "recs = np.array([])\n",
    "for var in varnames:\n",
    "    irec = np.where(fldlist == var)\n",
    "    recs = np.append(recs, irec[0][0])\n",
    "\n",
    "THETADR = np.full((len(tsstr),nz,ny,nx),np.nan)\n",
    "for i in range(len(tsstr)):\n",
    "    thisTHETADR,its,meta = rdmds(os.path.join(dirIn, file_name),int(tsstr[i]),returnmeta=True,rec=recs[0])\n",
    "    thisTHETADR = thisTHETADR.reshape(nz,ny,nx)\n",
    "    THETADR[i] = thisTHETADR\n",
    "\n",
    "THETADR =  (THETADR[1:, :, :,:] - THETADR[:-1, :,:, :]) / dt[:,np.newaxis,np.newaxis,np.newaxis]    # PSU.m/s    # degC.m/s\n",
    "THETADR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbfed303-bfef-471f-b9e1-29bd204bfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmptend=myparms['rcp']*(THETADR-AB_gT)* np.tile(RAC[np.newaxis,np.newaxis,:,:],(len(tsstr)-1,nz,1,1))  # J/m^3.degC * degC.m/s * m^2 = J/s\n",
    "budgO = {}\n",
    "budgI = {}\n",
    "budgO['heatfluxes'] = {}\n",
    "budgI['heatfluxes'] = {}\n",
    "\n",
    "budgO['heatfluxes']['tend'] = tmptend     # J/s\n",
    "\n",
    "budgO['heattend'] = np.nansum(tmptend,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5452bc8a-7fd7-4344-acae-003f8f0f3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "del THETADR, tmptend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffeff405-31a6-41b4-a162-c0c4a4c99718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files\n",
    "file_name = 'budg2d_zflux_set1'\n",
    "meta_budg2d_zflux_set1 = parsemeta(dirIn + file_name + \".\" + tsstr[0] + \".meta\")\n",
    "fldlist = np.array(meta_budg2d_zflux_set1[\"fldList\"])\n",
    "varnames = np.array([\"TFLUX\",\"oceQsw\",\"SItflux\"])\n",
    "recs = np.array([])\n",
    "for var in varnames:\n",
    "    irec = np.where(fldlist == var)\n",
    "    recs = np.append(recs, irec[0][0])\n",
    "\n",
    "TFLUX = np.zeros((len(tsstr)-1,ny,nx))\n",
    "oceQsw = np.zeros((len(tsstr)-1,ny,nx))\n",
    "SItflux = np.zeros((len(tsstr)-1,ny,nx))\n",
    "# loop and add to list\n",
    "for t in range(len(tsstr[1:])):\n",
    "    t2 = int(tsstr[t+1])  # +1 so we can read end of budg time steps\n",
    "    tmpTFLUX,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[0])\n",
    "    tmpoceQsw,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[1])\n",
    "    tmpSItflux,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[2])\n",
    "    TFLUX[t] = tmpTFLUX.reshape(ny,nx)\n",
    "    oceQsw[t] = tmpoceQsw.reshape(ny,nx)\n",
    "    SItflux[t] = tmpSItflux.reshape(ny,nx)\n",
    "\n",
    "# note: the following works provided that the first 3 terms are definitely there\n",
    "file_name = \"budg2d_zflux_set2\"\n",
    "meta_budg2d_zflux_set2 = parsemeta(dirIn + file_name + \".\" + tsstr[0] + \".meta\")\n",
    "fldlist = np.array(meta_budg2d_zflux_set2[\"fldList\"])\n",
    "varnames = np.array([\"oceQnet\",\"WTHMASS\",\"SIaaflux\",\"TRELAX\"])\n",
    "recs = np.array([])\n",
    "for var in varnames:\n",
    "    irec = np.where(fldlist == var)\n",
    "    recs = np.append(recs, irec[0][0])\n",
    "# read the files in a loop to get all timesteps\n",
    "oceQnet = np.zeros((len(tsstr)-1,ny,nx))\n",
    "WTHMASS = np.zeros((len(tsstr)-1,ny,nx))\n",
    "SIaaflux = np.zeros((len(tsstr)-1,ny,nx))\n",
    "TRELAX = np.zeros((len(tsstr)-1,ny,nx))\n",
    "for t in range(len(tsstr[1:])):\n",
    "    t2 = int(tsstr[t+1])\n",
    "    tmpoceQnet,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[0])\n",
    "    tmpWTHMASS,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[1])\n",
    "    tmpSIaaflux,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[2])\n",
    "    tmpTRELAX,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[3])\n",
    "    oceQnet[t] = tmpoceQnet.reshape(ny,nx)\n",
    "    WTHMASS[t] = tmpWTHMASS.reshape(ny,nx)\n",
    "    SIaaflux[t] = tmpSIaaflux.reshape(ny,nx)\n",
    "    TRELAX[t] = tmpTRELAX.reshape(ny,nx)\n",
    "\n",
    "# note: will not work if these are defined, fix for future steps\n",
    "varnames = np.array([\"TRELAX\",\"SIabflux\",\"SIacflux\",\"SIeprflx\",\"SIfldflx\"])\n",
    "recs = np.array([])\n",
    "for var in varnames:\n",
    "    irec = np.where(fldlist == var)\n",
    "    if len(irec[0]) > 0:\n",
    "        recs = np.append(recs, irec[0][0])\n",
    "# if len(recs) == 0:\n",
    "SIabflux = np.zeros((len(tsstr)-1,ny, nx))\n",
    "SIacflux = np.zeros((len(tsstr)-1,ny, nx))\n",
    "SIeprflx = np.zeros((len(tsstr)-1,ny, nx))\n",
    "SIfldflx = np.zeros((len(tsstr)-1,ny, nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae37c21-5f54-4b01-aba2-70faaf6e8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myparms['useNLFS'] == 0:\n",
    "    print('do nothing, already read above')\n",
    "else:\n",
    "    WTHMASS=0*WTHMASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abbdbb90-7540-458b-98f6-8b03fdcca608",
   "metadata": {},
   "outputs": [],
   "source": [
    "geothFlux = 0\n",
    "\n",
    "if myparms['SaltPlumeHeatFlux']:\n",
    "    print(1)\n",
    "else:\n",
    "    SPforcT1=0*np.ones((len(tsstr)-1,ny,nx))\n",
    "    oceEPtnd=0*np.ones((len(tsstr)-1,nz,ny,nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "837afd4c-bbe3-4ae6-b2ec-ea7676bd4856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read kpp tend and from 3d zflux\n",
    "file_name = \"budg3d_kpptend_set1\"\n",
    "meta_budg3d_kpptend_set1 = parsemeta(dirIn + file_name + \".\" + tsstr[0] + \".meta\")\n",
    "fldlist = np.array(meta_budg3d_kpptend_set1[\"fldList\"])\n",
    "varnames = np.array([\"KPPg_TH\"])\n",
    "recs = np.array([])\n",
    "for var in varnames:\n",
    "    irec = np.where(fldlist == var)\n",
    "    recs = np.append(recs, irec[0][0])\n",
    "# loop through and add to the list\n",
    "KPPg_TH = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "for t in range(len(tsstr[1:])):\n",
    "    t2 = int(tsstr[t+1])\n",
    "    tmpKPPg_TH,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[0])\n",
    "    KPPg_TH[t] = tmpKPPg_TH.reshape(nz,ny,nx)\n",
    "\n",
    "# now 3d zfluxes\n",
    "file_name = \"budg3d_zflux_set2\"\n",
    "meta_budg3d_zflux_set2 = parsemeta(dirIn + file_name + \".\" + tsstr[0] + \".meta\")\n",
    "fldlist = np.array(meta_budg3d_zflux_set2[\"fldList\"])\n",
    "varnames = np.array([\"ADVr_TH\",\"DFrE_TH\",\"DFrI_TH\"])\n",
    "recs = np.array([])\n",
    "for var in varnames:\n",
    "    irec = np.where(fldlist == var)\n",
    "    recs = np.append(recs, irec[0][0])\n",
    "# loop through the time steps and add\n",
    "ADVr_TH = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "DFrE_TH = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "DFrI_TH = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "for t in range(len(tsstr[1:])):\n",
    "    t2 = int(tsstr[t+1])\n",
    "\n",
    "    tmpADVr_TH,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[0])\n",
    "    tmpDFrE_TH,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[1])\n",
    "    tmpDFrI_TH,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[2])\n",
    "    ADVr_TH[t] = tmpADVr_TH.reshape(nz,ny,nx)\n",
    "    DFrE_TH[t] = tmpDFrE_TH.reshape(nz,ny,nx)\n",
    "    DFrI_TH[t] = tmpDFrI_TH.reshape(nz,ny,nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414b89f-aed4-47e9-8f9e-1511029678ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "budgO['heatzconv']=TFLUX+geothFlux+SPforcT1                           # W/m^2 = J/m^2/s\n",
    "zconv_top_heat = TFLUX  * RAC     # W/m^2 * m^2 = J/s\n",
    "budgI['heatzconv']=-(SItflux+TFLUX-TRELAX+SPforcT1)\n",
    "\n",
    "if myparms['useNLFS']==0:\n",
    "    budgO['heatzconv']=budgO['heatzconv']-myparms['rcp']*WTHMASS[:,:,:]     # degC.m/s * J/m^3degC = J/m^2.s\n",
    "\n",
    "budgI['heatzconv']=budgI['heatzconv']-SIabflux+SIacflux+SIeprflx\n",
    "if(myparms['SEAICEheatConsFix']==0):\n",
    "    budgI['heatzconv']=budgI['heatzconv']+SIaaflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3042ee-fb0b-49df-8598-abdfc9c51bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "del TFLUX,SPforcT1,WTHMASS,SIabflux,SIacflux,SIeprflx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f1764c-85b6-4bf4-a577-2968116332ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = mygrid['RC'].shape[0]\n",
    "trWtopADV = -(ADVr_TH) * myparms['rcp']         # J/s\n",
    "trWtopDF = -(DFrE_TH+DFrI_TH) * myparms['rcp']  # J/s\n",
    "trWtopKPP = -(KPPg_TH) * myparms['rcp']         # J/s\n",
    "trWtop = trWtopADV + trWtopDF + trWtopKPP       # J/s\n",
    "dd = mygrid['RF'][:-1]\n",
    "swfrac = 0.62*np.exp(dd/0.6)+(1-0.62)*np.exp(dd/20)\n",
    "swfrac[dd < -200] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6a33d-dd98-47d6-8286-f5a67790179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (np.tile(RAC[np.newaxis,:,:],(len(tsstr)-1,1,1)) * oceQsw)\n",
    "b = np.tile(swfrac[np.newaxis,:,np.newaxis,np.newaxis],(len(tsstr)-1,1,ny,nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc91974-dc71-442e-98bd-b66f8e501499",
   "metadata": {},
   "outputs": [],
   "source": [
    "swtop = b * np.tile(a[:,np.newaxis,:,:],(1,nz,1,1))   # J/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433d121-15f7-49b3-90fa-ede1a91da2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(swtop.shape,trWtop.shape)\n",
    "\n",
    "mskC=np.tile(mygrid['mskC'][np.newaxis,:,:,:],(len(tsstr)-1,1,1,1))\n",
    "print(mskC.shape)\n",
    "swtop[np.isnan(mskC)]=0\n",
    "trWtop=trWtop+swtop  # 323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd85ac16-ca82-4636-a0f5-d210266c75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trWtop[:,0,:,:]=budgO['heatzconv']*np.tile(RAC[np.newaxis,:,:],(len(tsstr)-1,1,1))\n",
    "trWbot = np.zeros_like(trWtop)\n",
    "trWbot[:,:-1,:,:]=trWtop[:,1:,:,:]\n",
    "\n",
    "budgO[\"heatfluxes\"][\"trWtop\"] = trWtop\n",
    "budgO[\"heatfluxes\"][\"trWbot\"] = trWbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aede14-bf02-4357-b148-c2fbbebbb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trWtop,trWbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e3a0d-12a1-45ba-92b2-7a34007eeac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "budgI[\"heatfluxes\"][\"trWtop\"] = -np.tile(RAC[np.newaxis,:,:],(len(tsstr)-1,1,1)) * (budgI[\"heatzconv\"] + budgO[\"heatzconv\"])\n",
    "budgI[\"heatfluxes\"][\"trWbot\"] = -np.tile(RAC[np.newaxis,:,:],(len(tsstr)-1,1,1)) * budgO[\"heatzconv\"]\n",
    "budgO['heatfluxes']['zconv']=budgO['heatfluxes']['trWtop']-budgO['heatfluxes']['trWbot']\n",
    "\n",
    "budgO['heatzconv'] = np.tile(RAC[np.newaxis,:,:],(len(tsstr)-1,1,1))*budgO['heatzconv']  # J/s\n",
    "budgI['heatzconv']=np.tile(RAC[np.newaxis,:,:],(len(tsstr)-1,1,1))*budgI['heatzconv']    # J/s\n",
    "# budgOI['heatzconv']=budgO['heatzconv']+budgI['heatzconv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af087af8-4d8d-4a97-a997-964687acbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read adv and dfe\n",
    "file_name = \"budg3d_hflux_set2\"\n",
    "meta_budg3d_hflux_set2 = parsemeta(dirIn + file_name + \".\" + tsstr[0] + \".meta\")\n",
    "fldlist = np.array(meta_budg3d_hflux_set2[\"fldList\"])\n",
    "varnames = np.array([\"ADVx_TH\",\"ADVy_TH\",\"DFxE_TH\",\"DFyE_TH\"])\n",
    "recs = np.array([])\n",
    "for var in varnames:\n",
    "    irec = np.where(fldlist == var)\n",
    "    recs = np.append(recs, irec[0][0])\n",
    "\n",
    "# do this onle for datetimes[1:] so we can set it in the file\n",
    "heatfluxeshconv = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "advfluxeshconv = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "dffluxeshconv = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "                     \n",
    "for t in range(len(tsstr[1:])):\n",
    "    print(t)\n",
    "    t2 = int(tsstr[t + 1])\n",
    "    # read the files\n",
    "    ADVx_TH,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[0])\n",
    "    ADVy_TH,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[1])\n",
    "    DFxE_TH,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[2])\n",
    "    DFyE_TH,its,meta = rdmds(os.path.join(dirIn, file_name),t2,returnmeta=True,rec=recs[3])\n",
    "\n",
    "    tmpUo = myparms['rcp'] * (ADVx_TH + DFxE_TH)\n",
    "    tmpVo = myparms['rcp'] * (ADVy_TH + DFyE_TH)\n",
    "    tmpUo = tmpUo.reshape(nz,ny,nx)\n",
    "    tmpVo = tmpVo.reshape(nz,ny,nx)\n",
    "    \n",
    "    # reshape and get the faces\n",
    "    tmpUo = get_aste_faces(tmpUo,nfx,nfy)              \n",
    "    tmpVo = get_aste_faces(tmpVo,nfx,nfy)\n",
    "\n",
    "    # set in the larger array\n",
    "    heatfluxeshconv[t] = calc_UV_conv_mod(nfx,nfy,tmpUo,tmpVo)\n",
    "\n",
    "    # also do for adv and df\n",
    "    tmpUo = get_aste_faces(ADVx_TH.reshape(nz,ny,nx),nfx,nfy)\n",
    "    tmpVo = get_aste_faces(ADVy_TH.reshape(nz,ny,nx),nfx,nfy)\n",
    "    advfluxeshconv[t] = calc_UV_conv_mod(nfx,nfy,tmpUo,tmpVo) * myparms['rcp']          #J/s\n",
    "\n",
    "    tmpUo = get_aste_faces(DFxE_TH.reshape(nz,ny,nx),nfx,nfy)\n",
    "    tmpVo = get_aste_faces(DFyE_TH.reshape(nz,ny,nx),nfx,nfy)\n",
    "    dffluxeshconv[t] = calc_UV_conv_mod(nfx,nfy,tmpUo,tmpVo) * myparms['rcp']      #J/s\n",
    "\n",
    "budgO['heatfluxes']['hconv'] = heatfluxeshconv\n",
    "budgO['heatfluxes']['ADV_hconv'] = advfluxeshconv\n",
    "budgO['heatfluxes']['DF_hconv'] = dffluxeshconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f6262-3e79-4985-9fd7-bdd4553bd52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del heatfluxeshconv,advfluxeshconv,dffluxeshconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4009064-b188-4720-8a5b-a755616b9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do vertical convergence for ADV and DF terms\n",
    "tmpadv = np.full((len(tsstr)-1,nz,ny,nx),np.nan)\n",
    "tmpadv[:,:-1,:,:] = (trWtopADV[:,:-1] - trWtopADV[:,1:])              # for surface thru seafloor\n",
    "\n",
    "Tconv = budgO['heatfluxes']['ADV_hconv'] + tmpadv\n",
    "budgO['heatfluxes']['ADV_Tconv'] = Tconv                            # J/s, this is the advective arrow of T for a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7703e2d-8fb2-4068-9620-1628d9c74c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Tconv,tmpadv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6718995-a140-425c-b750-fc6ac5ec024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do vertical convergence for ADV and DF terms\n",
    "tmpdf = np.full((len(tsstr)-1,nz,ny,nx),np.nan)\n",
    "tmpdf[:,:-1,:,:] = (trWtopDF[:,:-1] - trWtopDF[:,1:])              # for surface thru seafloor\n",
    "\n",
    "dfTconv = budgO['heatfluxes']['DF_hconv'] + tmpdf\n",
    "budgO['heatfluxes']['DF_Tconv'] = dfTconv      # J/s, this is the diffusive arrow of T for a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ed710-2960-421f-b1b4-5149d40ba0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfTconv,tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168560a-8d29-490d-83de-5e67697ec048",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpkpp = np.full((len(tsstr)-1,nz,ny,nx),np.nan)\n",
    "tmpkpp[:,:-1,:,:] = trWtopKPP[:,:-1] - trWtopKPP[:,1:]\n",
    "budgO['heatfluxes']['KPP_Tconv'] = tmpkpp        # no horizontal component for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9b5c7-b2a6-405b-a4c5-a20a74b79968",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tmpkpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b743a-bfa3-480d-b303-7a8d144c6366",
   "metadata": {},
   "source": [
    "# create an example time series OF SALT for a given set of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9eb6e8-5ed4-43cd-848f-845a35ef893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the bins of TS data\n",
    "# try new T bins where different sizes\n",
    "refined_section = np.linspace(-3,8,93)\n",
    "coarse_section = np.linspace(8,15,21,endpoint=False)\n",
    "binsTH_edges = np.concatenate((refined_section,coarse_section[1:]))\n",
    "binsTH_centers = (binsTH_edges[:-1] + binsTH_edges[1:])/2\n",
    "nT = binsTH_edges.shape[0]-1\n",
    "\n",
    "# do bi-sectional form for S\n",
    "coarse_section = np.linspace(0, 28, 30, endpoint=False)\n",
    "refined_section = np.linspace(28, 40, 83)\n",
    "binsSLT_edges = np.concatenate((coarse_section, refined_section))\n",
    "binsSLT_centers = (binsSLT_edges[:-1] + binsSLT_edges[1:])/2\n",
    "nS = binsSLT_edges.shape[0]-1\n",
    "\n",
    "Tbin,Sbin = np.meshgrid(binsTH_edges,binsSLT_edges)\n",
    "Tbincent,Sbincent = np.meshgrid(binsTH_centers,binsSLT_centers)\n",
    "\n",
    "binwidthT = binsTH_edges[1:] - binsTH_edges[:-1]\n",
    "binwidthS = binsSLT_edges[1:] - binsSLT_edges[:-1]\n",
    "dT,dS = np.meshgrid(binwidthT,binwidthS)\n",
    "dT = dT.reshape(112,112,1)\n",
    "dS = dS.reshape(112,112,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64aed3-7214-4bb2-b742-118e10793ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read theta and salt averages from the t2 timestep (average)\n",
    "# file_name = \"state_3d_set1\"\n",
    "# meta_budg3d_kpptend_set1 = parsemeta(dirState + file_name + \".\" + tsstr[0] + \".meta\")\n",
    "# fldlist = np.array(meta_budg3d_kpptend_set1[\"fldList\"])\n",
    "# varnames = np.array([\"THETA\",\"SALT\"])\n",
    "# recs = np.array([])\n",
    "# for var in varnames:\n",
    "#     irec = np.where(fldlist == var)\n",
    "#     recs = np.append(recs, irec[0][0])\n",
    "\n",
    "\n",
    "# THETA = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "# SALT = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "# binned_theta = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "# binned_salt = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "\n",
    "# # loop and add to list\n",
    "# for t in range(len(tsstr[1:])):\n",
    "#     t2 = int(tsstr[t+1])  # +1 so we can read end of budg time steps\n",
    "\n",
    "#     tmpTHETA,its,meta = rdmds(os.path.join(dirState, file_name),t2,returnmeta=True,rec=recs[0])\n",
    "#     tmpSALT,its,meta = rdmds(os.path.join(dirState, file_name),t2,returnmeta=True,rec=recs[1])\n",
    "#     tmpTHETA = tmpTHETA.reshape(nz,ny,nx)\n",
    "#     tmpSALT = tmpSALT.reshape(nz,ny,nx)\n",
    "\n",
    "#     # set in larger array\n",
    "#     THETA[t] = tmpTHETA\n",
    "#     SALT[t] = tmpSALT\n",
    "\n",
    "#     # bin for temp\n",
    "#     tmpbinned_theta = bin_array(tmpTHETA,binsTH_edges)\n",
    "#     tmpbinned_theta = tmpbinned_theta.astype(float)\n",
    "#     tmpbinned_theta[tmpbinned_theta == nT] = np.nan\n",
    "#     binned_theta[t] = tmpbinned_theta\n",
    "\n",
    "#     # also for salt\n",
    "#     tmpbinned_salt = bin_array(tmpSALT,binsSLT_edges)\n",
    "#     tmpbinned_salt = tmpbinned_salt.astype(float)\n",
    "#     tmpbinned_salt[tmpbinned_salt == nS] = np.nan\n",
    "#     binned_salt[t] = tmpbinned_salt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ded68-5d39-427c-a3d7-97c639350992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del THETA,SALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d6716-5d88-4fcb-bbf3-fc4986db2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # also make arrays of the binned_theta and binned_salt widths\n",
    "# default_value = 0\n",
    "# binwidthT_map = np.full_like(binned_theta, default_value, dtype=np.float64)\n",
    "# valid_mask = ~np.isnan(binned_theta)\n",
    "# binwidthT_map[valid_mask] = binwidthT[binned_theta[valid_mask].astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993bea9-be87-426a-b9b1-86433189ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for salt\n",
    "# binwidthS_map = np.full_like(binned_theta, default_value, dtype=np.float64)\n",
    "# binwidthS_map[valid_mask] = binwidthS[binned_salt[valid_mask].astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9c871-ac35-4551-a8f1-5270abab2b90",
   "metadata": {},
   "source": [
    "# create points, the example points at the surface in the Barents Sea, so we can do a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51571f83-bfa1-4f0b-8e35-0e61fa659877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some example points for the basin\n",
    "\n",
    "y_indices, x_indices = np.meshgrid(np.arange(ny), np.arange(nx), indexing='ij')\n",
    "condition = (mskBasin == iB)\n",
    "\n",
    "x_points = x_indices[condition]\n",
    "y_points = y_indices[condition]\n",
    "\n",
    "# # Combine the x and y points into a single array of shape (npoints, 2)\n",
    "points = np.vstack((y_points, x_points)).T\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440607e-d012-452f-8028-bb3e1334f4ba",
   "metadata": {},
   "source": [
    "# do the same J terms breakdown for theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94971e-9b7f-4cdd-8860-13f58a252f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "JtendTfull = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "JADVTfull = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "JDFTfull = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "JKPPTfull = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "JsurfTfull = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "budgetfull = np.zeros((len(tsstr)-1,nz,ny,nx))\n",
    "\n",
    "for t in range(len(tsstr)-1):\n",
    "    print(t)\n",
    "    \n",
    "    # show the vectors for each\n",
    "    aT = budgO['heatfluxes']['tend'][t]  # J/s \n",
    "    bT = budgO['heatfluxes']['ADV_Tconv'][t]\n",
    "    cT = budgO['heatfluxes']['DF_Tconv'][t]\n",
    "    dT = budgO['heatfluxes']['KPP_Tconv'][t]\n",
    "    eT = zconv_top_heat[t].reshape(1,ny,nx)\n",
    "    fT = swtop[t]\n",
    "\n",
    "    # translate this J terms for a column to the Jterms in 3D as a map\n",
    "\n",
    "    # save the J terms for here for the single point   \n",
    "    JtendT = (aT) / myparms['rcp']             # J/s / J/(m^3*degC) = degC.m^3/s\n",
    "    JADVT = (bT) / myparms['rcp']\n",
    "    JDFT = (cT) / myparms['rcp']\n",
    "    JKPPT = (dT) / myparms['rcp']\n",
    "\n",
    "    # do the surface term J vectors for the entire area\n",
    "    JsurfT = np.zeros((nz,ny,nx))                    # initialize the surface term\n",
    "    JsurfT[1:-1,:,:] += fT[2:,:,:] - fT[1:-1,:,:]    # add the convergence of the subsurface\n",
    "    JsurfT[0,:,:] += fT[1]   # init the surface of the surface term without the TFLUX\n",
    "    JsurfT[0,:,:] -= eT[0]\n",
    "    JsurfT = JsurfT / myparms['rcp']\n",
    "\n",
    "    # also do the budget based on the other terms\n",
    "    budget_ofJ = JtendT - JADVT - JDFT - JKPPT + JsurfT\n",
    "    #print(budget_ofJ.shape)\n",
    "\n",
    "    JtendTfull[t] = JtendT\n",
    "    JADVTfull[t] = JADVT\n",
    "    JDFTfull[t] = JDFT\n",
    "    JKPPTfull[t] = JKPPT\n",
    "    JsurfTfull[t] = JsurfT\n",
    "    budgetfull[t] = budget_ofJ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14a44b-04d5-4908-8326-ca556e9d69d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del budgO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffda63-3b7d-4464-87bb-8397a6fa1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write this to a dataset the first time\n",
    "# ds = xr.Dataset(\n",
    "#     {\n",
    "#         \"JtendTfull\": ([\"time\", \"z\", \"y\", \"x\"], JtendTfull)  # Replace 'variable_name' with the actual name\n",
    "#     },\n",
    "#     coords={\n",
    "#         \"time\": datetimes[:-1],\n",
    "#         \"z\": np.arange(nz),\n",
    "#         \"y\": np.arange(ny),\n",
    "#         \"x\": np.arange(nx),\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # add the other terms\n",
    "# ds['JADVTfull'] = ([\"time\", \"z\", \"y\", \"x\"], JADVTfull)\n",
    "# ds['JDFTfull'] = ([\"time\", \"z\", \"y\", \"x\"], JDFTfull)\n",
    "# ds['JKPPTfull'] = ([\"time\", \"z\", \"y\", \"x\"], JKPPTfull)\n",
    "# ds['JsurfTfull'] = ([\"time\", \"z\", \"y\", \"x\"], JsurfTfull)\n",
    "# ds['budgetfull'] = ([\"time\", \"z\", \"y\", \"x\"], budgetfull)\n",
    "\n",
    "# # write to a dataset (the first time, do not run again)\n",
    "# ds.to_netcdf(\"earlyyears_J.nc\", mode=\"w\", format=\"NETCDF4\", unlimited_dims=[\"time\"], engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad7cfd-0b02-413f-835c-03bebbb9638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the existing NetCDF file to append if necessary\n",
    "ds_old = xr.open_dataset(\"earlyyears_J.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224426-1fc5-4969-bc1c-dd91956297d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556bb63-fe56-4878-b9bb-cc6797dd7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the new data arrays into xarray DataArrays with appropriate dimensions\n",
    "ds_new = xr.Dataset(\n",
    "    {\n",
    "        \"JtendTfull\": ([\"time\", \"z\", \"y\", \"x\"], JtendTfull),  # Replace with new data arrays\n",
    "        \"JADVTfull\": ([\"time\", \"z\", \"y\", \"x\"], JADVTfull),\n",
    "        \"JDFTfull\": ([\"time\", \"z\", \"y\", \"x\"], JDFTfull),\n",
    "        \"JKPPTfull\": ([\"time\", \"z\", \"y\", \"x\"], JKPPTfull),\n",
    "        \"JsurfTfull\": ([\"time\", \"z\", \"y\", \"x\"], JsurfTfull),\n",
    "        \"budgetfull\": ([\"time\", \"z\", \"y\", \"x\"], budgetfull)\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": datetimes[:-1],  # New datetimes for the appended data\n",
    "        \"z\": np.arange(nz),     # Assuming z, y, x remain the same\n",
    "        \"y\": np.arange(ny),\n",
    "        \"x\": np.arange(nx),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Append the new data to the existing NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f9500-4de5-4676-9b86-b82f8d1bb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_combined = xr.concat([ds_old,ds_new],dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373eb34-5732-4f39-b7ca-7de993d23fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8414153-8c36-4783-8108-936053d0c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_combined.to_netcdf(\"earlyyears_J.nc\", mode=\"w\", format=\"NETCDF4\", unlimited_dims=[\"time\"], engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e38b87-fd74-402a-8f36-5b7bd1c9e21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700acd7-062d-4656-9896-82df6cad6e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f6459-4fd7-44ab-ac98-43105e5dd34c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f9b841c-3581-45a9-aadb-50a305b33e8b",
   "metadata": {},
   "source": [
    "# time series stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1c096-0e6b-49b6-b724-9fde0041f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"heatterms.txt\",\"a\") as inf:\n",
    "    \n",
    "    for t in range(len(tsstr)-1):\n",
    "        # get the values we want\n",
    "        a = np.nansum((JADVTfull[t] * mymsk3))\n",
    "        b = np.nansum((JDFTfull[t] * mymsk3))\n",
    "        c = np.nansum((JKPPTfull[t] * mymsk3))\n",
    "        d = np.nansum((JsurfTfull[t] * mymsk))\n",
    "        e = np.nansum((JtendTfull[t] * mymsk3))\n",
    "\n",
    "        # write to file\n",
    "        inf.write(f\"{datetimes[t]}\\n\")\n",
    "        inf.write(f\"{a}, {b}, {c}, {d}, {e}\\n\")\n",
    "        inf.write(f\"{a+b+c-d}\\n\")\n",
    "inf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41745e15-a0d0-43a0-90d4-91c8fb127731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from this array for later\n",
    "\n",
    "# Initialize lists to store the data\n",
    "dates = []\n",
    "abcde_values = []\n",
    "abc_minus_d_values = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(\"heatterms.txt\", \"r\") as inf:\n",
    "    while True:\n",
    "        # Read the date/time line\n",
    "        date_line = inf.readline().strip()\n",
    "        if not date_line:  # End of file\n",
    "            break\n",
    "        dates.append(date_line[:7])\n",
    "        \n",
    "        # Read the a, b, c, d, e values line\n",
    "        values_line = inf.readline().strip()\n",
    "        a, b, c, d, e = map(float, values_line.split(\", \"))\n",
    "        abcde_values.append([a, b, c, d, e])\n",
    "        \n",
    "        # Read the a+b+c-d value line\n",
    "        result_line = inf.readline().strip()\n",
    "        abc_minus_d_values.append(float(result_line))\n",
    "\n",
    "# Convert the lists to NumPy arrays for easier manipulation\n",
    "dates = np.array(dates)  # This will be an array of strings\n",
    "abcde_values = np.array(abcde_values)  # This will be a (n, 5) array\n",
    "abc_minus_d_values = np.array(abc_minus_d_values)  # This will be a (n,) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c05b08-316e-44a2-aa37-1afde5a94b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "abcde_values.shape  # adv, df, kpp, surface, tend\n",
    "abc_minus_d_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddcb0d-6683-4d31-a0a0-d36d614398ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.array(dates,dtype='datetime64[M]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14bca58-2819-4129-a7eb-5228d9d19af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can look at the time series of the salt terms\n",
    "\n",
    "# now we can look at a time series of each of the J terms for salt at this given cell in a given year\n",
    "\n",
    "fig = plt.figure(figsize=(10,9))\n",
    "plt.subplots_adjust(hspace=0.8)  # bigger hspace -- more vertical space\n",
    "\n",
    "plt.suptitle((\"Integrated Barents Sea Tendencies\"))\n",
    "\n",
    "ax = plt.subplot(611)\n",
    "ax.plot(dates,abcde_values[:,-1])\n",
    "coefficients = np.polyfit(np.arange(len(dates)), abcde_values[:,-1], 1)\n",
    "trendline = np.polyval(coefficients, np.arange(len(dates)))\n",
    "ax.plot(dates, trendline, label=\"Trendline\", color='red', linestyle='--')\n",
    "ax.set_title(\"heat tend\")\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"Sv/PSU\")\n",
    "num_ticks = len(dates)\n",
    "tick_indices = np.arange(0, num_ticks, 12)  # Indices of every 12th tick\n",
    "ax.set_xticks(dates[tick_indices])  # Set the positions of the ticks\n",
    "ax.set_xticklabels(dates[tick_indices].astype(str), rotation=15, ha='right')  # Set the labels and format\n",
    "\n",
    "\n",
    "ax = plt.subplot(612)\n",
    "ax.plot(dates,abcde_values[:,0])\n",
    "ax.set_title(\"ADV theta tend\")\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"Sv/PSU\")\n",
    "ax.set_xticks(dates[tick_indices])  # Set the positions of the ticks\n",
    "ax.set_xticklabels(dates[tick_indices].astype(str), rotation=15, ha='right')  # Set the labels and format\n",
    "\n",
    "ax = plt.subplot(613)\n",
    "ax.plot(dates,abcde_values[:,1])\n",
    "ax.set_title(\"DF theta tend\")\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"Sv/PSU\")\n",
    "ax.set_xticks(dates[tick_indices])  # Set the positions of the ticks\n",
    "ax.set_xticklabels(dates[tick_indices].astype(str), rotation=15, ha='right')  # Set the labels and format\n",
    "\n",
    "ax = plt.subplot(614)\n",
    "ax.plot(dates,abcde_values[:,2])\n",
    "ax.set_title(\"KPP theta tend\")\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"Sv/PSU\")\n",
    "ax.set_xticks(dates[tick_indices])  # Set the positions of the ticks\n",
    "ax.set_xticklabels(dates[tick_indices].astype(str), rotation=15, ha='right')  # Set the labels and format\n",
    "\n",
    "ax = plt.subplot(615)\n",
    "ax.plot(dates,abcde_values[:,3])\n",
    "ax.set_title(\"surf theta tend\")\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"Sv/PSU\")\n",
    "ax.set_xticks(dates[tick_indices])  # Set the positions of the ticks\n",
    "ax.set_xticklabels(dates[tick_indices].astype(str), rotation=15, ha='right')  # Set the labels and format\n",
    "\n",
    "ax = plt.subplot(616)\n",
    "ax.plot(dates,abc_minus_d_values)\n",
    "ax.set_title(\"tend from terms\")\n",
    "ax.grid()\n",
    "ax.set_ylabel(\"Sv/PSU\")\n",
    "ax.set_xticks(dates[tick_indices])  # Set the positions of the ticks\n",
    "ax.set_xticklabels(dates[tick_indices].astype(str), rotation=15, ha='right')  # Set the labels and format\n",
    "\n",
    "plt.savefig(path + \"BarentsSea_heatJterms_ts.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e7023-bbb3-4b10-8bed-728e6905f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba3304-212a-42c9-b231-5da62394da33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
